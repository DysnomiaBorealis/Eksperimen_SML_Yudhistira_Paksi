{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZLRMFl0JyyQ"
      },
      "source": [
        "# **1. Perkenalan Dataset**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hssSDn-5n3HR"
      },
      "source": [
        "Tahap pertama, Anda harus mencari dan menggunakan dataset dengan ketentuan sebagai berikut:\n",
        "\n",
        "1. **Sumber Dataset**:  \n",
        "   Dataset dapat diperoleh dari berbagai sumber, seperti public repositories (*Kaggle*, *UCI ML Repository*, *Open Data*) atau data primer yang Anda kumpulkan sendiri.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKADPWcFKlj3"
      },
      "source": [
        "# **2. Import Library**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgA3ERnVn84N"
      },
      "source": [
        "Pada tahap ini, Anda perlu mengimpor beberapa pustaka (library) Python yang dibutuhkan untuk analisis data dan pembangunan model machine learning atau deep learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlmvjLY9M4Yj"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For text preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "\n",
        "print('Libraries imported successfully!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **3. Memuat Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pada tahap ini, Anda perlu memuat dataset ke dalam notebook. Jika dataset dalam format CSV, Anda bisa menggunakan pustaka pandas untuk membacanya. Pastikan untuk mengecek beberapa baris awal dataset untuk memahami strukturnya dan memastikan data telah dimuat dengan benar.\n",
        "\n",
        "Jika dataset berada di Google Drive, pastikan Anda menghubungkan Google Drive ke Colab terlebih dahulu. Setelah dataset berhasil dimuat, langkah berikutnya adalah memeriksa kesesuaian data dan siap untuk dianalisis lebih lanjut.\n",
        "\n",
        "Jika dataset berupa unstructured data, silakan sesuaikan dengan format seperti kelas Machine Learning Pengembangan atau Machine Learning Terapan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3YIEnAFKrKL"
      },
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('indo_spam.csv')\n",
        "\n",
        "# Display basic information\n",
        "print('Dataset loaded successfully!')\n",
        "print(f'Dataset shape: {df.shape}')\n",
        "print(f'Columns: {list(df.columns)}')\n",
        "print('\\nFirst 5 rows:')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgZkbJLpK9UR"
      },
      "source": [
        "# **4. Exploratory Data Analysis (EDA)**\n",
        "\n",
        "Pada tahap ini, Anda akan melakukan **Exploratory Data Analysis (EDA)** untuk memahami karakteristik dataset.\n",
        "\n",
        "Tujuan dari EDA adalah untuk memperoleh wawasan awal yang mendalam mengenai data dan menentukan langkah selanjutnya dalam analisis atau pemodelan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cek Info Dataset, duplikat data dan missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "print('=== Dataset Information ===')\n",
        "df.info()\n",
        "print('\\n=== Statistical Summary ===')\n",
        "print(df.describe())\n",
        "print('\\n=== Missing Values ===')\n",
        "print(df.isnull().sum())\n",
        "print(f'\\n=== Duplicate Rows: {df.duplicated().sum()} ===')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cek distribusi kelas dan visualisasinya"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\n=== Class Distribution ===')\n",
        "print(df['Kategori'].value_counts())\n",
        "\n",
        "# Visualisasi\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(data=df, x='Kategori', palette='viridis')\n",
        "plt.title('Distribution of Spam vs Ham Messages', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Category', fontsize=12)\n",
        "plt.ylabel('Count', fontsize=12)\n",
        "plt.xticks(rotation=0)\n",
        "for i, v in enumerate(df['Kategori'].value_counts()):\n",
        "    plt.text(i, v + 50, str(v), ha='center', va='bottom', fontsize=11)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cek analysis seperti panjangnya messange, melakukan visualisasi dan word count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analisa panjangnya pesan\n",
        "df['message_length'] = df['Pesan'].apply(lambda x: len(str(x)))\n",
        "df['word_count'] = df['Pesan'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "print('\\n=== Message Length Statistics ===')\n",
        "print(df.groupby('Kategori')[['message_length', 'word_count']].describe())\n",
        "\n",
        "# Visualisasi dari distribusi panjangnya pesan\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Visualisasi dari panjangnya pesan\n",
        "df.boxplot(column='message_length', by='Kategori', ax=axes[0])\n",
        "axes[0].set_title('Message Length by Category')\n",
        "axes[0].set_xlabel('Category')\n",
        "axes[0].set_ylabel('Character Count')\n",
        "plt.sca(axes[0])\n",
        "plt.xticks(rotation=0)\n",
        "\n",
        "# Visualisasi Word count\n",
        "df.boxplot(column='word_count', by='Kategori', ax=axes[1])\n",
        "axes[1].set_title('Word Count by Category')\n",
        "axes[1].set_xlabel('Category')\n",
        "axes[1].set_ylabel('Word Count')\n",
        "plt.sca(axes[1])\n",
        "plt.xticks(rotation=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Sample messages\n",
        "print('\\n=== Sample Spam Messages ===')\n",
        "print(df[df['Kategori'] == 'Spam']['Pesan'].head(3).values)\n",
        "print('\\n=== Sample Ham Messages ===')\n",
        "print(df[df['Kategori'] == 'ham']['Pesan'].head(3).values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpgHfgnSK3ip"
      },
      "source": [
        "# **5. Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COf8KUPXLg5r"
      },
      "source": [
        "Pada tahap ini, data preprocessing adalah langkah penting untuk memastikan kualitas data sebelum digunakan dalam model machine learning.\n",
        "\n",
        "Jika Anda menggunakan data teks, data mentah sering kali mengandung nilai kosong, duplikasi, atau rentang nilai yang tidak konsisten, yang dapat memengaruhi kinerja model. Oleh karena itu, proses ini bertujuan untuk membersihkan dan mempersiapkan data agar analisis berjalan optimal.\n",
        "\n",
        "Berikut adalah tahapan-tahapan yang bisa dilakukan, tetapi **tidak terbatas** pada:\n",
        "1. Menghapus atau Menangani Data Kosong (Missing Values)\n",
        "2. Menghapus Data Duplikat\n",
        "3. Normalisasi atau Standarisasi Fitur\n",
        "4. Deteksi dan Penanganan Outlier\n",
        "5. Encoding Data Kategorikal\n",
        "6. Binning (Pengelompokan Data)\n",
        "\n",
        "Cukup sesuaikan dengan karakteristik data yang kamu gunakan yah. Khususnya ketika kami menggunakan data tidak terstruktur."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Menangani Data Kosong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Handle missing values\n",
        "df = df.dropna()\n",
        "print(f'After removing missing values: {len(df)} rows')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Menghapus Data Duplikat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f'Before removing duplicates: {len(df)} rows')\n",
        "df = df.drop_duplicates()\n",
        "print(f'After removing duplicates: {len(df)} rows')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Dilakukan data encoding (kategorical) dan standarisasi fitur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    # Convert ke lowercase\n",
        "    text = str(text).lower()\n",
        "    \n",
        "    # hapus urls\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "    \n",
        "    # hapus email addresses\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)\n",
        "    \n",
        "    # hapus phone numbers\n",
        "    text = re.sub(r'\\d{5,}', '', text)\n",
        "    \n",
        "    # hapus special characters dan digits\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "    \n",
        "    # hapus whitespace  \n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "# apply text cleaning\n",
        "print('\\nCleaning text...')\n",
        "df['cleaned_text'] = df['Pesan'].apply(clean_text)\n",
        "\n",
        "# sesudah dan sebelum text cleaning\n",
        "print('\\n=== Text Cleaning Examples ===')\n",
        "for i in range(3):\n",
        "    print(f'\\nOriginal: {df.iloc[i][\"Pesan\"]}')\n",
        "    print(f'Cleaned: {df.iloc[i][\"cleaned_text\"]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Encode Target Variable   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "df['label'] = df['Kategori'].map({'Spam': 1, 'ham': 0})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. Data Binning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Split data ke training dan testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df['cleaned_text']\n",
        "y = df['label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f'\\n=== Train-Test Split ===')\n",
        "print(f'Training set: {len(X_train)} samples')\n",
        "print(f'Testing set: {len(X_test)} samples')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vektorisasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\nPerforming TF-IDF vectorization...')\n",
        "vectorizer = TfidfVectorizer(max_features=3000, min_df=2, max_df=0.8)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "print(f'TF-IDF matrix shape (train): {X_train_tfidf.shape}')\n",
        "print(f'TF-IDF matrix shape (test): {X_test_tfidf.shape}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Simpan data yang sudah dilakukan preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocessed_df = df[['Kategori', 'Pesan', 'cleaned_text', 'label']].copy()\n",
        "preprocessed_df.to_csv('preprocessing/indo_spam_preprocessing.csv', index=False)\n",
        "print('\\nPreprocessed data saved to: preprocessing/indo_spam_preprocessing.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6. Validasi model untuk training cepat, prediksi, evaluasi serta Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Og8pGV0-iDLz"
      },
      "outputs": [],
      "source": [
        "print('\\n=== Training a Quick Naive Bayes Model for Validation ===')\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Prediksi\n",
        "y_pred = nb_model.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluasi\n",
        "print(f'\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}')\n",
        "print('\\nClassification Report:')\n",
        "print(classification_report(y_test, y_pred, target_names=['Ham', 'Spam']))\n",
        "\n",
        "# Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])\n",
        "plt.title('Confusion Matrix - Naive Bayes Classifier', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Actual', fontsize=12)\n",
        "plt.xlabel('Predicted', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\\n=== Data Preprocessing Completed! ===')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
